{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-04T23:04:33.980531Z","iopub.execute_input":"2024-08-04T23:04:33.981292Z","iopub.status.idle":"2024-08-04T23:04:34.455648Z","shell.execute_reply.started":"2024-08-04T23:04:33.981257Z","shell.execute_reply":"2024-08-04T23:04:34.454503Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# *Text Reduction*\n___\n#### This project aims to reduce texts from the internet using NLP techniques.","metadata":{}},{"cell_type":"markdown","source":"# Packages :","metadata":{}},{"cell_type":"code","source":"# !pip install beautifulsoup4","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:04:35.251911Z","iopub.execute_input":"2024-08-04T23:04:35.252987Z","iopub.status.idle":"2024-08-04T23:04:35.257346Z","shell.execute_reply.started":"2024-08-04T23:04:35.252949Z","shell.execute_reply":"2024-08-04T23:04:35.256246Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import  word_tokenize\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\nimport string\n\nstopwords = nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:04:38.926833Z","iopub.execute_input":"2024-08-04T23:04:38.927257Z","iopub.status.idle":"2024-08-04T23:04:40.669748Z","shell.execute_reply.started":"2024-08-04T23:04:38.927222Z","shell.execute_reply":"2024-08-04T23:04:40.668602Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:04:44.309688Z","iopub.execute_input":"2024-08-04T23:04:44.310352Z","iopub.status.idle":"2024-08-04T23:04:44.319601Z","shell.execute_reply.started":"2024-08-04T23:04:44.310289Z","shell.execute_reply":"2024-08-04T23:04:44.318398Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"stopwords.append(\"href=\")\nstopwords.append(\"class=\")\nstopwords.append(\"id=\")\nstopwords.append(\"title=\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:04:45.206562Z","iopub.execute_input":"2024-08-04T23:04:45.207391Z","iopub.status.idle":"2024-08-04T23:04:45.213072Z","shell.execute_reply.started":"2024-08-04T23:04:45.207320Z","shell.execute_reply":"2024-08-04T23:04:45.211671Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"ponc = string.punctuation + \"  '' ,\\\"\\\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:39:05.239374Z","iopub.execute_input":"2024-08-04T23:39:05.240435Z","iopub.status.idle":"2024-08-04T23:39:05.245595Z","shell.execute_reply.started":"2024-08-04T23:39:05.240399Z","shell.execute_reply":"2024-08-04T23:39:05.244404Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"ponc","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:39:07.636609Z","iopub.execute_input":"2024-08-04T23:39:07.637012Z","iopub.status.idle":"2024-08-04T23:39:07.644039Z","shell.execute_reply.started":"2024-08-04T23:39:07.636977Z","shell.execute_reply":"2024-08-04T23:39:07.642895Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~  \\'\\' ,\"\"'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Get articals:","metadata":{}},{"cell_type":"code","source":"url = \"https://en.wikipedia.org/wiki/Liquid_state_machine\"\nresponce = requests.get(url)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:39:08.452057Z","iopub.execute_input":"2024-08-04T23:39:08.452494Z","iopub.status.idle":"2024-08-04T23:39:08.492422Z","shell.execute_reply.started":"2024-08-04T23:39:08.452460Z","shell.execute_reply":"2024-08-04T23:39:08.491249Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"html = responce.text\n\nhtml_obj = BeautifulSoup(html,'html.parser')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:39:10.425665Z","iopub.execute_input":"2024-08-04T23:39:10.426791Z","iopub.status.idle":"2024-08-04T23:39:10.475577Z","shell.execute_reply.started":"2024-08-04T23:39:10.426742Z","shell.execute_reply":"2024-08-04T23:39:10.474476Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"all_paragraph = html_obj.find_all(\"p\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:39:11.214125Z","iopub.execute_input":"2024-08-04T23:39:11.214525Z","iopub.status.idle":"2024-08-04T23:39:11.220740Z","shell.execute_reply.started":"2024-08-04T23:39:11.214494Z","shell.execute_reply":"2024-08-04T23:39:11.219508Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"pattren = \"\\<\\/?.{1,4}\\>\"\nall_paragraph = [re.sub(pattren,\"\",str(i)) for i in all_paragraph]","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:39:11.984441Z","iopub.execute_input":"2024-08-04T23:39:11.985400Z","iopub.status.idle":"2024-08-04T23:39:11.992301Z","shell.execute_reply.started":"2024-08-04T23:39:11.985355Z","shell.execute_reply":"2024-08-04T23:39:11.991126Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"all_paragraph = [word_tokenize(txt.lower()) for txt in all_paragraph]","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:39:12.396477Z","iopub.execute_input":"2024-08-04T23:39:12.396879Z","iopub.status.idle":"2024-08-04T23:39:12.410467Z","shell.execute_reply.started":"2024-08-04T23:39:12.396846Z","shell.execute_reply":"2024-08-04T23:39:12.409320Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"all_paragraph = [[word for word in txt if word not in ponc] for txt in all_paragraph]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:39:13.109817Z","iopub.execute_input":"2024-08-04T23:39:13.110303Z","iopub.status.idle":"2024-08-04T23:39:13.115774Z","shell.execute_reply.started":"2024-08-04T23:39:13.110272Z","shell.execute_reply":"2024-08-04T23:39:13.114687Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"all_paragraph = [[word for word in txt if word not in stopwords] for txt in all_paragraph]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:39:15.060136Z","iopub.execute_input":"2024-08-04T23:39:15.060534Z","iopub.status.idle":"2024-08-04T23:39:15.066754Z","shell.execute_reply.started":"2024-08-04T23:39:15.060503Z","shell.execute_reply":"2024-08-04T23:39:15.065545Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"word_fr = {}\n\nfor txt in all_paragraph:\n    for word in txt:\n        if word in word_fr.keys():\n            word_fr[word] += 1\n        else:\n            word_fr[word] = 1","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:42:16.590604Z","iopub.execute_input":"2024-08-04T23:42:16.591015Z","iopub.status.idle":"2024-08-04T23:42:16.597082Z","shell.execute_reply.started":"2024-08-04T23:42:16.590984Z","shell.execute_reply":"2024-08-04T23:42:16.596012Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"max_val = max(word_fr.values())","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:44:41.409013Z","iopub.execute_input":"2024-08-04T23:44:41.409455Z","iopub.status.idle":"2024-08-04T23:44:41.414614Z","shell.execute_reply.started":"2024-08-04T23:44:41.409418Z","shell.execute_reply":"2024-08-04T23:44:41.413275Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"word_fr_nrml = {}\nfor key,val in word_fr.items():\n    word_fr_nrml[key] = val/max_val","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:47:20.125116Z","iopub.execute_input":"2024-08-04T23:47:20.125535Z","iopub.status.idle":"2024-08-04T23:47:20.131035Z","shell.execute_reply.started":"2024-08-04T23:47:20.125503Z","shell.execute_reply":"2024-08-04T23:47:20.129866Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def weghts_extracting(url):\n    \n    responce = requests.get(url)\n    \n    html = responce.text\n    html_obj = BeautifulSoup(html,'html.parser')\n    \n    # find all paragraphs\n    all_paragraph = html_obj.find_all(\"p\")\n    \n    # remove html tags\n    pattren = \"\\<\\/?.{1,4}\\>\"\n    all_paragraph = [re.sub(pattren,\"\",str(i)) for i in all_paragraph]\n    \n    # tokenize words\n    all_paragraph = [word_tokenize(txt.lower()) for txt in all_paragraph]\n    \n    # remove punctuation\n    all_paragraph = [[word for word in txt if word not in ponc] for txt in all_paragraph]\n    \n    # remove stopwords\n    all_paragraph = [[word for word in txt if word not in stopwords] for txt in all_paragraph]\n    \n    # words freq\n    word_fr = {}\n\n    for txt in all_paragraph:\n        for word in txt:\n            if word in word_fr.keys():\n                word_fr[word] += 1\n            else:\n                word_fr[word] = 1\n                \n    # get max freq\n    max_val = max(word_fr.values())\n    \n    # normalizeation\n    word_fr_nrml = {}\n    for key,val in word_fr.items():\n        word_fr_nrml[key] = val/max_val\n        \n    return word_fr_nrml\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:52:47.168176Z","iopub.execute_input":"2024-08-04T23:52:47.168634Z","iopub.status.idle":"2024-08-04T23:52:47.180058Z","shell.execute_reply.started":"2024-08-04T23:52:47.168601Z","shell.execute_reply":"2024-08-04T23:52:47.178777Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}